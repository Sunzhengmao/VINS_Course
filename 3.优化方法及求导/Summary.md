## 总结最优化方法
- 在系统的优化方法中主要有两类问题：**约束优化和无约束优化**，一个是在优化时有约束限制，一个没有；在slam问题中以无约束优化为主;
- 在无约束优化中，主要有两种思路：**line-search和信赖域法**，lin-search的思路很直接，先找到理想的下降方向，再确定前进步长；而信赖域则是先给一个约束
- line-search的**一阶优化**方法中主要是**梯度下降法**。因为梯度方向是下降最快的方向，所以以当前点的梯度方向作为前进方向，在计算步长的时候可以算精确步长，也可以为了减少计算量用一个近似值，但是无论哪种情况都必须保证整体损失的下降。
- 如果参数量少，计算能力足够，我们会更多选择**二阶算法**，比如**牛顿法**，牛顿法是说在最优值附近中用二次函数来拟合，并一步一步找到最优值，而且收敛速度会快很多，因为看到的东西多了，但是二阶必然会带来计算量的问题。后面**高斯牛顿法**为了解决这个问题，使用JTJ来代替二阶的Hessian矩阵，同时也应用了最小二乘这样的优化结构，但是如果JTJ不满秩同样无法求逆；所以**LM算法**就在JTJ后面加了个u倍单位阵，而且可以通过控制u的大小使优化算法在高斯牛顿和梯度下降之间转换；
- 信赖域法中的**DogLeg**，我事先给他一个范围让他在这里面优化，如果直接优化到最优值那就直接结束，否则的话，我就得通过近似函数和真实函数的逼近程度来判断（真实下降量和预计下降量之比），如果逼近的很好，而且也到了边界，那我认为是我的边界限制了他，我就要把范围调大；如果逼近的不好，我就得把范围调小，防止跳过了最优值。所以在迭代的过程中需要及时的动态调整信赖域半径，直到达到最优值

- 各个方法的约束：
高斯牛顿：①小残量问题（离最优点近）；②非线性程度比较小；
H秩亏（不可逆，有可能出现极限时秩亏）时用LM算法，但是这得保证是半正定（离最优点近——
H负定时候可以用信赖域法
line-search：从差步恢复的方式沿着参数曲线回退；信赖域：考虑整个求步的过程，非常强的大范围收敛性